{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "explanations": [
     "This is the main title and introduction for the project notebook."
    ]
   },
   "source": [
    "# <p style=\"background-color:#003366; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;\">Customer Conversion Prediction with ML & DL</p>\n",
    "\n",
    "This notebook provides a comprehensive workflow for binary classification on a digital marketing dataset. The goal is to predict whether a customer will convert. It covers a wide range of models, from classical machine learning to advanced boosting methods and a Keras-based deep neural network. Each model is trained and evaluated systematically to compare their performance."
   ],
   "id": "intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "explanations": [
     "This cell imports all the necessary Python libraries for the entire workflow, organized by their function."
    ]
   },
   "source": [
    "## 1. Library Imports"
   ],
   "id": "imports_header"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:20.053385Z",
     "start_time": "2025-09-25T15:53:13.762162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Preprocessing & Splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Classical ML Models\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "# Ensemble Models\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "\n",
    "# Advanced Boosting Models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Deep Learning with Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "import optuna\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Libraries imported successfully.\")"
   ],
   "id": "e2adbdf9538f3625",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "explanations": [
     "Header for the data loading and cleaning section."
    ]
   },
   "source": [
    "## 2. Data Loading & Cleaning"
   ],
   "id": "loading_header"
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "This cell reads the dataset from a CSV file into a pandas DataFrame. It then removes columns that are constant or act as unique identifiers, as they provide no predictive value for the models. Finally, it separates the data into features (X) and the target variable (y)."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:20.139403Z",
     "start_time": "2025-09-25T15:53:20.074879Z"
    }
   },
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('digital_marketing_campaign_dataset.csv')\n",
    "\n",
    "# Drop columns that do not add predictive value\n",
    "df.drop(['CustomerID', 'AdvertisingPlatform', 'AdvertisingTool'], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop('Conversion', axis=1)\n",
    "y = df['Conversion']\n",
    "\n",
    "print(\"Data loaded and cleaned. Feature and target sets created.\")\n",
    "display(X.head())"
   ],
   "id": "loading_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and cleaned. Feature and target sets created.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Age  Gender  Income CampaignChannel CampaignType      AdSpend  \\\n",
       "0   56  Female  136912    Social Media    Awareness  6497.870068   \n",
       "1   69    Male   41760           Email    Retention  3898.668606   \n",
       "2   46  Female   88456             PPC    Awareness  1546.429596   \n",
       "3   32  Female   44085             PPC   Conversion   539.525936   \n",
       "4   60  Female   83964             PPC   Conversion  1678.043573   \n",
       "\n",
       "   ClickThroughRate  ConversionRate  WebsiteVisits  PagesPerVisit  TimeOnSite  \\\n",
       "0          0.043919        0.088031              0       2.399017    7.396803   \n",
       "1          0.155725        0.182725             42       2.917138    5.352549   \n",
       "2          0.277490        0.076423              2       8.223619   13.794901   \n",
       "3          0.137611        0.088004             47       4.540939   14.688363   \n",
       "4          0.252851        0.109940              0       2.046847   13.993370   \n",
       "\n",
       "   SocialShares  EmailOpens  EmailClicks  PreviousPurchases  LoyaltyPoints  \n",
       "0            19           6            9                  4            688  \n",
       "1             5           2            7                  2           3459  \n",
       "2             0          11            2                  8           2337  \n",
       "3            89           2            2                  0           2463  \n",
       "4             6           6            6                  8           4345  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Income</th>\n",
       "      <th>CampaignChannel</th>\n",
       "      <th>CampaignType</th>\n",
       "      <th>AdSpend</th>\n",
       "      <th>ClickThroughRate</th>\n",
       "      <th>ConversionRate</th>\n",
       "      <th>WebsiteVisits</th>\n",
       "      <th>PagesPerVisit</th>\n",
       "      <th>TimeOnSite</th>\n",
       "      <th>SocialShares</th>\n",
       "      <th>EmailOpens</th>\n",
       "      <th>EmailClicks</th>\n",
       "      <th>PreviousPurchases</th>\n",
       "      <th>LoyaltyPoints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>Female</td>\n",
       "      <td>136912</td>\n",
       "      <td>Social Media</td>\n",
       "      <td>Awareness</td>\n",
       "      <td>6497.870068</td>\n",
       "      <td>0.043919</td>\n",
       "      <td>0.088031</td>\n",
       "      <td>0</td>\n",
       "      <td>2.399017</td>\n",
       "      <td>7.396803</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>Male</td>\n",
       "      <td>41760</td>\n",
       "      <td>Email</td>\n",
       "      <td>Retention</td>\n",
       "      <td>3898.668606</td>\n",
       "      <td>0.155725</td>\n",
       "      <td>0.182725</td>\n",
       "      <td>42</td>\n",
       "      <td>2.917138</td>\n",
       "      <td>5.352549</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>Female</td>\n",
       "      <td>88456</td>\n",
       "      <td>PPC</td>\n",
       "      <td>Awareness</td>\n",
       "      <td>1546.429596</td>\n",
       "      <td>0.277490</td>\n",
       "      <td>0.076423</td>\n",
       "      <td>2</td>\n",
       "      <td>8.223619</td>\n",
       "      <td>13.794901</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>44085</td>\n",
       "      <td>PPC</td>\n",
       "      <td>Conversion</td>\n",
       "      <td>539.525936</td>\n",
       "      <td>0.137611</td>\n",
       "      <td>0.088004</td>\n",
       "      <td>47</td>\n",
       "      <td>4.540939</td>\n",
       "      <td>14.688363</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>Female</td>\n",
       "      <td>83964</td>\n",
       "      <td>PPC</td>\n",
       "      <td>Conversion</td>\n",
       "      <td>1678.043573</td>\n",
       "      <td>0.252851</td>\n",
       "      <td>0.109940</td>\n",
       "      <td>0</td>\n",
       "      <td>2.046847</td>\n",
       "      <td>13.993370</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>4345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "explanations": [
     "Header for the train/test split section."
    ]
   },
   "source": [
    "## 3. Train/Test Split"
   ],
   "id": "split_header"
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "This cell splits the data into a training set (for teaching the models) and a testing set (for final, unbiased evaluation). Stratification is used to ensure that the proportion of 'Conversion' and 'No Conversion' samples is the same in both the train and test sets, which is crucial for imbalanced datasets."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:20.221942Z",
     "start_time": "2025-09-25T15:53:20.204803Z"
    }
   },
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ],
   "id": "split_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (6400, 16)\n",
      "Test set shape: (1600, 16)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "explanations": [
     "Header for the data preprocessing section."
    ]
   },
   "source": [
    "## 4. Preprocessing Pipeline"
   ],
   "id": "preprocess_header"
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "This cell defines the preprocessing steps for the data. It first identifies which columns are numerical and which are categorical. Then, it creates a 'ColumnTransformer' which is a recipe for how to treat each type of column. Categorical columns are converted into numbers using One-Hot Encoding, and numerical columns are scaled to a standard range using StandardScaler. This ensures all features are in a format the models can understand."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:20.418284Z",
     "start_time": "2025-09-25T15:53:20.404286Z"
    }
   },
   "source": [
    "# Identify categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_features = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Create the preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create a separate preprocessor for Naive Bayes models that require non-negative data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "preprocessor_nb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "print(\"Preprocessing pipelines created.\")"
   ],
   "id": "preprocess_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipelines created.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "explanations": [
     "Main header for the model training and evaluation part of the notebook. Each subsequent cell will focus on a specific model."
    ]
   },
   "source": [
    "## 5. Model Training & Evaluation"
   ],
   "id": "training_header"
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a Logistic Regression model. A pipeline is used to combine the preprocessing steps and the model into a single workflow. The model is trained on the training data, and then its performance is measured on the unseen test data using multiple key classification metrics."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:20.540663Z",
     "start_time": "2025-09-25T15:53:20.468144Z"
    }
   },
   "source": [
    "print(\"--- 1. Logistic Regression ---\")\n",
    "pipeline_lr = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LogisticRegression(random_state=42))])\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "y_pred = pipeline_lr.predict(X_test)\n",
    "y_proba = pipeline_lr.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "lr_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Logistic Regression ---\n",
      "Accuracy: 0.8912\n",
      "Precision: 0.8946\n",
      "Recall: 0.9929\n",
      "F1 Score: 0.9412\n",
      "ROC-AUC: 0.7850\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a Ridge Classifier. This is a linear model similar to Logistic Regression but with L2 regularization, which helps prevent overfitting by penalizing large coefficient values."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:20.607796Z",
     "start_time": "2025-09-25T15:53:20.563352Z"
    }
   },
   "source": [
    "print(\"\\n--- 2. Ridge Classifier ---\")\n",
    "pipeline_ridge = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', RidgeClassifier(random_state=42))])\n",
    "pipeline_ridge.fit(X_train, y_train)\n",
    "y_pred = pipeline_ridge.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "# RidgeClassifier does not have predict_proba, so ROC-AUC cannot be calculated directly\n",
    "print(\"ROC-AUC: N/A\")"
   ],
   "id": "ridge_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Ridge Classifier ---\n",
      "Accuracy: 0.8762\n",
      "Precision: 0.8762\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9340\n",
      "ROC-AUC: N/A\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates an SGDClassifier (Stochastic Gradient Descent). It's a versatile model that can be configured for different linear models. Here it's used with a 'log' loss function, making it behave similarly to Logistic Regression but trained with a different optimization algorithm."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:20.738150Z",
     "start_time": "2025-09-25T15:53:20.617009Z"
    }
   },
   "source": [
    "print(\"\\n--- 3. SGDClassifier ---\")\n",
    "pipeline_sgd = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', SGDClassifier(loss='log_loss', random_state=42))])\n",
    "pipeline_sgd.fit(X_train, y_train)\n",
    "y_pred = pipeline_sgd.predict(X_test)\n",
    "y_proba = pipeline_sgd.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "sgd_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. SGDClassifier ---\n",
      "Accuracy: 0.8862\n",
      "Precision: 0.9050\n",
      "Recall: 0.9722\n",
      "F1 Score: 0.9374\n",
      "ROC-AUC: 0.7703\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a Linear Discriminant Analysis (LDA) model. LDA is a classifier that works by finding a linear combination of features that best separates the two classes."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:20.822884Z",
     "start_time": "2025-09-25T15:53:20.748016Z"
    }
   },
   "source": [
    "print(\"\\n--- 4. Linear Discriminant Analysis (LDA) ---\")\n",
    "pipeline_lda = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LinearDiscriminantAnalysis())])\n",
    "pipeline_lda.fit(X_train, y_train)\n",
    "y_pred = pipeline_lda.predict(X_test)\n",
    "y_proba = pipeline_lda.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "lda_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Linear Discriminant Analysis (LDA) ---\n",
      "Accuracy: 0.8888\n",
      "Precision: 0.8913\n",
      "Recall: 0.9943\n",
      "F1 Score: 0.9400\n",
      "ROC-AUC: 0.7833\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a Quadratic Discriminant Analysis (QDA) model. QDA is similar to LDA but allows for a more flexible, quadratic decision boundary between classes, which can be useful if the classes are not linearly separable."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:20.889894Z",
     "start_time": "2025-09-25T15:53:20.832598Z"
    }
   },
   "source": [
    "print(\"\\n--- 5. Quadratic Discriminant Analysis (QDA) ---\")\n",
    "pipeline_qda = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', QuadraticDiscriminantAnalysis())])\n",
    "pipeline_qda.fit(X_train, y_train)\n",
    "y_pred = pipeline_qda.predict(X_test)\n",
    "y_proba = pipeline_qda.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "qda_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Quadratic Discriminant Analysis (QDA) ---\n",
      "Accuracy: 0.7963\n",
      "Precision: 0.8865\n",
      "Recall: 0.8802\n",
      "F1 Score: 0.8833\n",
      "ROC-AUC: 0.5085\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a Gaussian Naive Bayes model. This probabilistic model is based on Bayes' theorem and assumes that numerical features follow a Gaussian (normal) distribution. It's known for being fast and simple."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:20.977610Z",
     "start_time": "2025-09-25T15:53:20.899553Z"
    }
   },
   "source": [
    "print(\"\\n--- 6.1. Gaussian Naive Bayes ---\")\n",
    "pipeline_gnb = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', GaussianNB())])\n",
    "pipeline_gnb.fit(X_train, y_train)\n",
    "y_pred = pipeline_gnb.predict(X_test)\n",
    "y_proba = pipeline_gnb.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "gnb_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6.1. Gaussian Naive Bayes ---\n",
      "Accuracy: 0.8919\n",
      "Precision: 0.8927\n",
      "Recall: 0.9964\n",
      "F1 Score: 0.9417\n",
      "ROC-AUC: 0.7923\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a Multinomial Naive Bayes model. This variant of Naive Bayes is typically used for text classification with word counts, but it can be applied to other data with discrete features. It requires non-negative feature values, so a special preprocessor with MinMaxScaler is used."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:21.049500Z",
     "start_time": "2025-09-25T15:53:20.988942Z"
    }
   },
   "source": [
    "print(\"\\n--- 6.2. Multinomial Naive Bayes ---\")\n",
    "pipeline_mnb = Pipeline(steps=[('preprocessor', preprocessor_nb), ('classifier', MultinomialNB())])\n",
    "pipeline_mnb.fit(X_train, y_train)\n",
    "y_pred = pipeline_mnb.predict(X_test)\n",
    "y_proba = pipeline_mnb.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "mnb_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6.2. Multinomial Naive Bayes ---\n",
      "Accuracy: 0.8762\n",
      "Precision: 0.8762\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9340\n",
      "ROC-AUC: 0.7112\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a Bernoulli Naive Bayes model. This version is designed for binary/boolean features (features that are either present or absent). The one-hot encoded features make this a suitable model to test."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:21.107902Z",
     "start_time": "2025-09-25T15:53:21.058620Z"
    }
   },
   "source": [
    "print(\"\\n--- 6.3. Bernoulli Naive Bayes ---\")\n",
    "pipeline_bnb = Pipeline(steps=[('preprocessor', preprocessor_nb), ('classifier', BernoulliNB())])\n",
    "pipeline_bnb.fit(X_train, y_train)\n",
    "y_pred = pipeline_bnb.predict(X_test)\n",
    "y_proba = pipeline_bnb.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "bnb_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6.3. Bernoulli Naive Bayes ---\n",
      "Accuracy: 0.8775\n",
      "Precision: 0.8773\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9347\n",
      "ROC-AUC: 0.6347\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a Decision Tree Classifier. This model learns a set of if-then-else rules to make predictions, creating a tree-like structure. It's highly interpretable but can be prone to overfitting if not constrained."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:21.317574Z",
     "start_time": "2025-09-25T15:53:21.126984Z"
    }
   },
   "source": [
    "print(\"\\n--- 7. Decision Tree Classifier ---\")\n",
    "pipeline_dt = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', DecisionTreeClassifier(random_state=42))])\n",
    "pipeline_dt.fit(X_train, y_train)\n",
    "y_pred = pipeline_dt.predict(X_test)\n",
    "y_proba = pipeline_dt.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "dt_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 7. Decision Tree Classifier ---\n",
      "Accuracy: 0.8337\n",
      "Precision: 0.9086\n",
      "Recall: 0.9009\n",
      "F1 Score: 0.9047\n",
      "ROC-AUC: 0.6297\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a Random Forest Classifier. This is an ensemble model that builds many decision trees on different random subsets of the data and features. It combines their predictions (by voting) to produce a more accurate and robust result than a single tree."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:22.232553Z",
     "start_time": "2025-09-25T15:53:21.326347Z"
    }
   },
   "source": [
    "print(\"\\n--- 8. Random Forest Classifier ---\")\n",
    "pipeline_rf = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))])\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "y_pred = pipeline_rf.predict(X_test)\n",
    "y_proba = pipeline_rf.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "rf_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 8. Random Forest Classifier ---\n",
      "Accuracy: 0.8862\n",
      "Precision: 0.8875\n",
      "Recall: 0.9964\n",
      "F1 Score: 0.9388\n",
      "ROC-AUC: 0.7933\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates an Extra Trees Classifier (Extremely Randomized Trees). It's similar to a Random Forest but introduces more randomness in how splits are chosen for the trees, which can sometimes lead to better performance and reduced variance."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:26.617844Z",
     "start_time": "2025-09-25T15:53:25.671979Z"
    }
   },
   "source": [
    "print(\"\\n--- 9. Extra Trees Classifier ---\")\n",
    "pipeline_et = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', ExtraTreesClassifier(random_state=42, n_jobs=-1))])\n",
    "pipeline_et.fit(X_train, y_train)\n",
    "y_pred = pipeline_et.predict(X_test)\n",
    "y_proba = pipeline_et.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "et_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 9. Extra Trees Classifier ---\n",
      "Accuracy: 0.8788\n",
      "Precision: 0.8784\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9353\n",
      "ROC-AUC: 0.8067\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a Bagging Classifier. Bagging (Bootstrap Aggregating) is the general ensemble technique that Random Forest is based on. It builds multiple instances of a base estimator (like a Decision Tree) on random subsets of the data and aggregates their predictions."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:38.332417Z",
     "start_time": "2025-09-25T15:53:26.650855Z"
    }
   },
   "source": [
    "print(\"\\n--- 10. Bagging Classifier ---\")\n",
    "pipeline_bag = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', BaggingClassifier(random_state=42, n_jobs=-1))])\n",
    "pipeline_bag.fit(X_train, y_train)\n",
    "y_pred = pipeline_bag.predict(X_test)\n",
    "y_proba = pipeline_bag.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "bag_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 10. Bagging Classifier ---\n",
      "Accuracy: 0.8925\n",
      "Precision: 0.9095\n",
      "Recall: 0.9743\n",
      "F1 Score: 0.9408\n",
      "ROC-AUC: 0.7694\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates an AdaBoost Classifier (Adaptive Boosting). This is a boosting algorithm that builds models sequentially, with each new model focusing more on the samples that the previous models got wrong."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:39.441793Z",
     "start_time": "2025-09-25T15:53:38.627857Z"
    }
   },
   "source": [
    "print(\"\\n--- 11. AdaBoost Classifier ---\")\n",
    "pipeline_ada = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', AdaBoostClassifier(random_state=42))])\n",
    "pipeline_ada.fit(X_train, y_train)\n",
    "y_pred = pipeline_ada.predict(X_test)\n",
    "y_proba = pipeline_ada.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "ada_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 11. AdaBoost Classifier ---\n",
      "Accuracy: 0.8994\n",
      "Precision: 0.9001\n",
      "Recall: 0.9957\n",
      "F1 Score: 0.9455\n",
      "ROC-AUC: 0.8238\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a Gradient Boosting Classifier. This is a powerful boosting technique where new models are trained to predict the errors (residuals) of the previous models, leading to a highly accurate ensemble."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:42.847375Z",
     "start_time": "2025-09-25T15:53:39.641086Z"
    }
   },
   "source": [
    "print(\"\\n--- 12. Gradient Boosting Classifier ---\")\n",
    "pipeline_gb = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', GradientBoostingClassifier(random_state=42))])\n",
    "pipeline_gb.fit(X_train, y_train)\n",
    "y_pred = pipeline_gb.predict(X_test)\n",
    "y_proba = pipeline_gb.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "gb_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 12. Gradient Boosting Classifier ---\n",
      "Accuracy: 0.9100\n",
      "Precision: 0.9127\n",
      "Recall: 0.9922\n",
      "F1 Score: 0.9508\n",
      "ROC-AUC: 0.8167\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a K-Nearest Neighbors (KNN) Classifier. This model classifies a new data point based on the majority class of its 'k' closest neighbors in the feature space. It's a simple yet effective non-parametric method."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:44.909647Z",
     "start_time": "2025-09-25T15:53:43.083127Z"
    }
   },
   "source": [
    "print(\"\\n--- 13. K-Nearest Neighbors (KNN) ---\")\n",
    "pipeline_knn = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', KNeighborsClassifier(n_jobs=-1))])\n",
    "pipeline_knn.fit(X_train, y_train)\n",
    "y_pred = pipeline_knn.predict(X_test)\n",
    "y_proba = pipeline_knn.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "knn_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 13. K-Nearest Neighbors (KNN) ---\n",
      "Accuracy: 0.8806\n",
      "Precision: 0.8854\n",
      "Recall: 0.9922\n",
      "F1 Score: 0.9358\n",
      "ROC-AUC: 0.6465\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a Linear Support Vector Classifier (LinearSVC). This is a version of Support Vector Machines that uses a linear kernel. It's highly efficient and works well for linearly separable data."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:45.238260Z",
     "start_time": "2025-09-25T15:53:45.178931Z"
    }
   },
   "source": [
    "print(\"\\n--- 14.1. LinearSVC ---\")\n",
    "pipeline_lsvc = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LinearSVC(random_state=42))])\n",
    "pipeline_lsvc.fit(X_train, y_train)\n",
    "y_pred = pipeline_lsvc.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "# LinearSVC does not have predict_proba, so ROC-AUC cannot be calculated directly\n",
    "print(\"ROC-AUC: N/A\")"
   ],
   "id": "lsvc_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 14.1. LinearSVC ---\n",
      "Accuracy: 0.8812\n",
      "Precision: 0.8811\n",
      "Recall: 0.9993\n",
      "F1 Score: 0.9365\n",
      "ROC-AUC: N/A\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a Support Vector Classifier (SVC) with a Radial Basis Function (RBF) kernel. The RBF kernel allows the SVM to find complex, non-linear decision boundaries, making it a very powerful classifier."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:53:52.989549Z",
     "start_time": "2025-09-25T15:53:45.283560Z"
    }
   },
   "source": [
    "print(\"\\n--- 14.2. SVC with RBF Kernel ---\")\n",
    "pipeline_svc_rbf = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', SVC(kernel='rbf', probability=True, random_state=42))])\n",
    "pipeline_svc_rbf.fit(X_train, y_train)\n",
    "y_pred = pipeline_svc_rbf.predict(X_test)\n",
    "y_proba = pipeline_svc_rbf.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "svc_rbf_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 14.2. SVC with RBF Kernel ---\n",
      "Accuracy: 0.8906\n",
      "Precision: 0.8925\n",
      "Recall: 0.9950\n",
      "F1 Score: 0.9410\n",
      "ROC-AUC: 0.7878\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates an MLPClassifier, which is a simple feedforward neural network from scikit-learn. It consists of multiple layers of nodes and can learn highly complex, non-linear patterns in the data."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:54:05.739523Z",
     "start_time": "2025-09-25T15:53:53.270790Z"
    }
   },
   "source": [
    "print(\"\\n--- 15. MLPClassifier (Neural Network) ---\")\n",
    "pipeline_mlp = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', MLPClassifier(random_state=42, max_iter=500))])\n",
    "pipeline_mlp.fit(X_train, y_train)\n",
    "y_pred = pipeline_mlp.predict(X_test)\n",
    "y_proba = pipeline_mlp.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "mlp_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 15. MLPClassifier (Neural Network) ---\n",
      "Accuracy: 0.8606\n",
      "Precision: 0.9102\n",
      "Recall: 0.9330\n",
      "F1 Score: 0.9215\n",
      "ROC-AUC: 0.7517\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates an XGBoost Classifier. XGBoost (eXtreme Gradient Boosting) is a highly optimized and powerful implementation of the gradient boosting algorithm, known for its speed and top-tier performance in many competitions."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:54:06.504402Z",
     "start_time": "2025-09-25T15:54:06.009033Z"
    }
   },
   "source": [
    "print(\"\\n--- 16. XGBoost Classifier ---\")\n",
    "pipeline_xgb = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))])\n",
    "pipeline_xgb.fit(X_train, y_train)\n",
    "y_pred = pipeline_xgb.predict(X_test)\n",
    "y_proba = pipeline_xgb.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "xgb_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 16. XGBoost Classifier ---\n",
      "Accuracy: 0.9106\n",
      "Precision: 0.9177\n",
      "Recall: 0.9864\n",
      "F1 Score: 0.9508\n",
      "ROC-AUC: 0.7991\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a LightGBM Classifier. LightGBM is another high-performance gradient boosting framework that is particularly known for its speed and efficiency, especially on large datasets. It grows trees leaf-wise instead of level-wise, which can lead to faster convergence."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:54:06.928890Z",
     "start_time": "2025-09-25T15:54:06.516857Z"
    }
   },
   "source": [
    "print(\"\\n--- 17. LightGBM Classifier ---\")\n",
    "pipeline_lgb = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', lgb.LGBMClassifier(random_state=42, n_jobs=-1))])\n",
    "pipeline_lgb.fit(X_train, y_train)\n",
    "y_pred = pipeline_lgb.predict(X_test)\n",
    "y_proba = pipeline_lgb.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "lgb_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 17. LightGBM Classifier ---\n",
      "[LightGBM] [Info] Number of positive: 5610, number of negative: 790\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001317 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2055\n",
      "[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 24\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.876563 -> initscore=1.960273\n",
      "[LightGBM] [Info] Start training from score 1.960273\n",
      "Accuracy: 0.9144\n",
      "Precision: 0.9192\n",
      "Recall: 0.9893\n",
      "F1 Score: 0.9529\n",
      "ROC-AUC: 0.8158\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Trains and evaluates a CatBoost Classifier. CatBoost is a gradient boosting library that excels at handling categorical features automatically and effectively. It uses a special technique called ordered boosting to reduce overfitting."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:54:15.771908Z",
     "start_time": "2025-09-25T15:54:06.952998Z"
    }
   },
   "source": [
    "print(\"\\n--- 18. CatBoost Classifier ---\")\n",
    "pipeline_cat = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', cb.CatBoostClassifier(random_state=42, verbose=0))])\n",
    "pipeline_cat.fit(X_train, y_train)\n",
    "y_pred = pipeline_cat.predict(X_test)\n",
    "y_proba = pipeline_cat.predict_proba(X_test)[:, 1]\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "cat_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 18. CatBoost Classifier ---\n",
      "Accuracy: 0.9237\n",
      "Precision: 0.9261\n",
      "Recall: 0.9922\n",
      "F1 Score: 0.9580\n",
      "ROC-AUC: 0.8179\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "Builds, trains, and evaluates a Deep Learning model using Keras. First, the data is preprocessed. Then, a Sequential neural network is defined with multiple Dense (fully connected) layers, ReLU activation functions to introduce non-linearity, and Dropout layers to prevent overfitting. The model is compiled with the Adam optimizer and binary cross-entropy loss, and trained with an early stopping mechanism to prevent unnecessary training."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:54:30.707405Z",
     "start_time": "2025-09-25T15:54:16.143447Z"
    }
   },
   "source": [
    "print(\"\\n--- 19. Keras Deep Learning Dense Model ---\")\n",
    "# Preprocess data specifically for the Keras model\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Define the model architecture\n",
    "keras_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_processed.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "keras_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "keras_model.fit(X_train_processed, y_train, epochs=100, batch_size=32, \n",
    "                validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "y_proba = keras_model.predict(X_test_processed).ravel()\n",
    "y_pred = (y_proba > 0.5).astype(int)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "keras_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 19. Keras Deep Learning Dense Model ---\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "Accuracy: 0.8944\n",
      "Precision: 0.9080\n",
      "Recall: 0.9786\n",
      "F1 Score: 0.9420\n",
      "ROC-AUC: 0.7679\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "explanations": [
     "Header for the ensembling section, where multiple models are combined to improve performance."
    ]
   },
   "source": [
    "## 6. Ensembling"
   ],
   "id": "ensemble_header"
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "This cell builds and evaluates a Voting Classifier, which is an ensemble that combines predictions from multiple models. The 'estimators' are the individual models we want to include. 'Hard' voting means the final prediction is based on a majority vote. 'Soft' voting averages the predicted probabilities from each model, which is often more powerful."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:54:35.161325Z",
     "start_time": "2025-09-25T15:54:30.982940Z"
    }
   },
   "source": [
    "print(\"\\n--- 7.1. Voting Classifier ---\")\n",
    "# Select a few diverse, well-performing models for the ensemble\n",
    "estimators = [\n",
    "    ('lr', LogisticRegression(random_state=42)),\n",
    "    ('rf', RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
    "    ('xgb', xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
    "]\n",
    "\n",
    "# Hard Voting\n",
    "pipeline_vote_hard = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                                     ('classifier', VotingClassifier(estimators=estimators, voting='hard', n_jobs=-1))])\n",
    "pipeline_vote_hard.fit(X_train, y_train)\n",
    "y_pred_hard = pipeline_vote_hard.predict(X_test)\n",
    "print(\"Hard Voting Results:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, y_pred_hard):.4f}\")\n",
    "print(f\"  F1 Score: {f1_score(y_test, y_pred_hard):.4f}\")\n",
    "\n",
    "# Soft Voting\n",
    "pipeline_vote_soft = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                                     ('classifier', VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1))])\n",
    "pipeline_vote_soft.fit(X_train, y_train)\n",
    "y_pred_soft = pipeline_vote_soft.predict(X_test)\n",
    "y_proba_soft = pipeline_vote_soft.predict_proba(X_test)[:, 1]\n",
    "print(\"\\nSoft Voting Results:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, y_pred_soft):.4f}\")\n",
    "print(f\"  F1 Score: {f1_score(y_test, y_pred_soft):.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_proba_soft):.4f}\")"
   ],
   "id": "voting_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 7.1. Voting Classifier ---\n",
      "Hard Voting Results:\n",
      "  Accuracy: 0.8931\n",
      "  F1 Score: 0.9422\n",
      "\n",
      "Soft Voting Results:\n",
      "  Accuracy: 0.9019\n",
      "  F1 Score: 0.9467\n",
      "  ROC-AUC: 0.8070\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "This cell builds and evaluates a Stacking Classifier. Stacking is a more advanced ensemble method where the predictions of several base models (estimators) are used as input for a final 'meta-learner' (in this case, Logistic Regression). The meta-learner learns how to best combine the base models' predictions to make a final, often more accurate, prediction."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:54:42.467022Z",
     "start_time": "2025-09-25T15:54:35.610997Z"
    }
   },
   "source": [
    "print(\"\\n--- 7.2. Stacking Classifier ---\")\n",
    "stacking_estimators = [\n",
    "    ('rf', RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
    "    ('xgb', xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')),\n",
    "    ('lgbm', lgb.LGBMClassifier(random_state=42, n_jobs=-1))\n",
    "]\n",
    "\n",
    "pipeline_stack = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                                 ('classifier', StackingClassifier(estimators=stacking_estimators, \n",
    "                                                                   final_estimator=LogisticRegression(), \n",
    "                                                                   cv=5, n_jobs=-1))])\n",
    "pipeline_stack.fit(X_train, y_train)\n",
    "y_pred_stack = pipeline_stack.predict(X_test)\n",
    "y_proba_stack = pipeline_stack.predict_proba(X_test)[:, 1]\n",
    "print(\"Stacking Results:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
    "print(f\"  F1 Score: {f1_score(y_test, y_pred_stack):.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_proba_stack):.4f}\")"
   ],
   "id": "stacking_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 7.2. Stacking Classifier ---\n",
      "Stacking Results:\n",
      "  Accuracy: 0.9225\n",
      "  F1 Score: 0.9570\n",
      "  ROC-AUC: 0.8099\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "explanations": [
     "Header for the advanced hyperparameter tuning section using Optuna."
    ]
   },
   "source": [
    "## 7. Advanced Hyperparameter Tuning with Optuna"
   ],
   "id": "optuna_header"
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "This cell performs hyperparameter optimization for the XGBoost model using Optuna. An 'objective' function is defined, which takes a trial object, defines the hyperparameter search space, creates a pipeline, and returns the cross-validated accuracy. Optuna then intelligently searches this space for 50 trials to find the combination of parameters that yields the best accuracy."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T15:55:38.053036Z",
     "start_time": "2025-09-25T15:54:43.160277Z"
    }
   },
   "source": [
    "print(\"\\n--- 8.1. Optuna for XGBoost ---\")\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'random_state': 42,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "    \n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', xgb.XGBClassifier(**params))])\n",
    "    \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    score = cross_val_score(pipeline, X_train, y_train, n_jobs=-1, cv=3, scoring='accuracy').mean()\n",
    "    return score\n",
    "\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=50, timeout=600) # Run for 50 trials or 10 minutes\n",
    "\n",
    "print(f\"Best XGBoost Trial Score: {study_xgb.best_value:.4f}\")\n",
    "print(f\"Best XGBoost Params: {study_xgb.best_params}\")\n",
    "\n",
    "# Retrain and evaluate with best params\n",
    "best_params_xgb = study_xgb.best_params\n",
    "pipeline_xgb_opt = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss', **best_params_xgb))])\n",
    "pipeline_xgb_opt.fit(X_train, y_train)\n",
    "y_pred = pipeline_xgb_opt.predict(X_test)\n",
    "y_proba = pipeline_xgb_opt.predict_proba(X_test)[:, 1]\n",
    "print(\"\\nOptimized XGBoost Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "optuna_xgb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-25 21:24:43,171] A new study created in memory with name: no-name-a499ce64-7778-4e9c-a2e3-28e8f38bd442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 8.1. Optuna for XGBoost ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-25 21:24:45,013] Trial 0 finished with value: 0.9003126660049537 and parameters: {'n_estimators': 267, 'max_depth': 7, 'learning_rate': 0.011017191423363734, 'subsample': 0.9521524621035461, 'colsample_bytree': 0.7929892535880235, 'gamma': 1.7585997485431781}. Best is trial 0 with value: 0.9003126660049537.\n",
      "[I 2025-09-25 21:24:46,166] Trial 1 finished with value: 0.9110934039160582 and parameters: {'n_estimators': 650, 'max_depth': 8, 'learning_rate': 0.059970478428753055, 'subsample': 0.8992087146559469, 'colsample_bytree': 0.8763953828261434, 'gamma': 1.4878821255578782}. Best is trial 1 with value: 0.9110934039160582.\n",
      "[I 2025-09-25 21:24:48,850] Trial 2 finished with value: 0.908124849053119 and parameters: {'n_estimators': 591, 'max_depth': 7, 'learning_rate': 0.03414655306543619, 'subsample': 0.6245515163542303, 'colsample_bytree': 0.6295375263093302, 'gamma': 0.09051270115771282}. Best is trial 1 with value: 0.9110934039160582.\n",
      "[I 2025-09-25 21:24:49,975] Trial 3 finished with value: 0.9134377398764716 and parameters: {'n_estimators': 481, 'max_depth': 6, 'learning_rate': 0.0460493247992628, 'subsample': 0.7676188870756322, 'colsample_bytree': 0.7933671589363573, 'gamma': 1.6372301886604468}. Best is trial 3 with value: 0.9134377398764716.\n",
      "[I 2025-09-25 21:24:51,222] Trial 4 finished with value: 0.9101570755622693 and parameters: {'n_estimators': 896, 'max_depth': 4, 'learning_rate': 0.1753620977111268, 'subsample': 0.6510311992614264, 'colsample_bytree': 0.6069737853838991, 'gamma': 1.2745215819016609}. Best is trial 3 with value: 0.9134377398764716.\n",
      "[I 2025-09-25 21:24:51,777] Trial 5 finished with value: 0.9109377153441707 and parameters: {'n_estimators': 105, 'max_depth': 6, 'learning_rate': 0.048349035049627696, 'subsample': 0.6453321978423022, 'colsample_bytree': 0.8997820808553256, 'gamma': 3.119204119410102}. Best is trial 3 with value: 0.9134377398764716.\n",
      "[I 2025-09-25 21:24:53,418] Trial 6 finished with value: 0.907812812832605 and parameters: {'n_estimators': 344, 'max_depth': 10, 'learning_rate': 0.039375912271728035, 'subsample': 0.8917493379291404, 'colsample_bytree': 0.6162076343941681, 'gamma': 0.4528903869767814}. Best is trial 3 with value: 0.9134377398764716.\n",
      "[I 2025-09-25 21:24:54,200] Trial 7 finished with value: 0.908124849053119 and parameters: {'n_estimators': 468, 'max_depth': 10, 'learning_rate': 0.2578654639102316, 'subsample': 0.8871367748087895, 'colsample_bytree': 0.8665952265097544, 'gamma': 1.54657534455528}. Best is trial 3 with value: 0.9134377398764716.\n",
      "[I 2025-09-25 21:24:56,661] Trial 8 finished with value: 0.9087503128783742 and parameters: {'n_estimators': 762, 'max_depth': 10, 'learning_rate': 0.01608406139700607, 'subsample': 0.9975395558597038, 'colsample_bytree': 0.9869050263299947, 'gamma': 2.2872121355030317}. Best is trial 3 with value: 0.9134377398764716.\n",
      "[I 2025-09-25 21:24:57,390] Trial 9 finished with value: 0.9078125931403586 and parameters: {'n_estimators': 465, 'max_depth': 8, 'learning_rate': 0.24224728633137377, 'subsample': 0.8697350732956285, 'colsample_bytree': 0.6609921184498522, 'gamma': 2.3307667930431992}. Best is trial 3 with value: 0.9134377398764716.\n",
      "[I 2025-09-25 21:24:58,551] Trial 10 finished with value: 0.9184374960180781 and parameters: {'n_estimators': 984, 'max_depth': 3, 'learning_rate': 0.10569097248545897, 'subsample': 0.7409902139733191, 'colsample_bytree': 0.7335884422440451, 'gamma': 4.82399731869694}. Best is trial 10 with value: 0.9184374960180781.\n",
      "[I 2025-09-25 21:24:59,768] Trial 11 finished with value: 0.920781246132501 and parameters: {'n_estimators': 997, 'max_depth': 3, 'learning_rate': 0.0965883506732652, 'subsample': 0.7376107557080175, 'colsample_bytree': 0.7350964663683712, 'gamma': 4.813529868830518}. Best is trial 11 with value: 0.920781246132501.\n",
      "[I 2025-09-25 21:25:00,914] Trial 12 finished with value: 0.9193755086790887 and parameters: {'n_estimators': 999, 'max_depth': 3, 'learning_rate': 0.10917637441407758, 'subsample': 0.7430146319593836, 'colsample_bytree': 0.71854268148185, 'gamma': 4.940856015326605}. Best is trial 11 with value: 0.920781246132501.\n",
      "[I 2025-09-25 21:25:01,844] Trial 13 finished with value: 0.9195314901739714 and parameters: {'n_estimators': 830, 'max_depth': 4, 'learning_rate': 0.11066916404807574, 'subsample': 0.7119713485113026, 'colsample_bytree': 0.7011063910622057, 'gamma': 4.870831671153931}. Best is trial 11 with value: 0.920781246132501.\n",
      "[I 2025-09-25 21:25:03,751] Trial 14 finished with value: 0.9209381796271178 and parameters: {'n_estimators': 822, 'max_depth': 4, 'learning_rate': 0.09256958871193528, 'subsample': 0.7011510449447179, 'colsample_bytree': 0.7224244806361596, 'gamma': 3.9603185509021213}. Best is trial 14 with value: 0.9209381796271178.\n",
      "[I 2025-09-25 21:25:04,950] Trial 15 finished with value: 0.9157818561446384 and parameters: {'n_estimators': 730, 'max_depth': 5, 'learning_rate': 0.07745305367330378, 'subsample': 0.8169105678352816, 'colsample_bytree': 0.7525879476225923, 'gamma': 3.9217522556074034}. Best is trial 14 with value: 0.9209381796271178.\n",
      "[I 2025-09-25 21:25:06,171] Trial 16 finished with value: 0.9198445516249684 and parameters: {'n_estimators': 894, 'max_depth': 4, 'learning_rate': 0.15705163597359112, 'subsample': 0.689849727528574, 'colsample_bytree': 0.6722873895746126, 'gamma': 3.8712834941741816}. Best is trial 14 with value: 0.9209381796271178.\n",
      "[I 2025-09-25 21:25:07,479] Trial 17 finished with value: 0.915313032891005 and parameters: {'n_estimators': 720, 'max_depth': 5, 'learning_rate': 0.026625401255311685, 'subsample': 0.8079861663357196, 'colsample_bytree': 0.8208387958351016, 'gamma': 3.959769423981892}. Best is trial 14 with value: 0.9209381796271178.\n",
      "[I 2025-09-25 21:25:08,636] Trial 18 finished with value: 0.9204694296042332 and parameters: {'n_estimators': 877, 'max_depth': 3, 'learning_rate': 0.07771477374530428, 'subsample': 0.6859756747001043, 'colsample_bytree': 0.7578826430907583, 'gamma': 3.1530806009614913}. Best is trial 14 with value: 0.9209381796271178.\n",
      "[I 2025-09-25 21:25:09,641] Trial 19 finished with value: 0.9185941365896997 and parameters: {'n_estimators': 996, 'max_depth': 5, 'learning_rate': 0.16511699011858713, 'subsample': 0.7781858420310203, 'colsample_bytree': 0.679074638962475, 'gamma': 4.226303503261398}. Best is trial 14 with value: 0.9209381796271178.\n",
      "[I 2025-09-25 21:25:10,675] Trial 20 finished with value: 0.9207816855169937 and parameters: {'n_estimators': 807, 'max_depth': 4, 'learning_rate': 0.0676299826074362, 'subsample': 0.6005230140833938, 'colsample_bytree': 0.8298862633988254, 'gamma': 3.2529329343430136}. Best is trial 14 with value: 0.9209381796271178.\n",
      "[I 2025-09-25 21:25:11,687] Trial 21 finished with value: 0.9181255330283126 and parameters: {'n_estimators': 807, 'max_depth': 4, 'learning_rate': 0.07124746773886906, 'subsample': 0.6048138634105755, 'colsample_bytree': 0.8316037254042299, 'gamma': 3.2116956119604714}. Best is trial 14 with value: 0.9209381796271178.\n",
      "[I 2025-09-25 21:25:12,428] Trial 22 finished with value: 0.9198438925482293 and parameters: {'n_estimators': 647, 'max_depth': 3, 'learning_rate': 0.09874172796143971, 'subsample': 0.6788404581561263, 'colsample_bytree': 0.9392860606149673, 'gamma': 4.353489554924542}. Best is trial 14 with value: 0.9209381796271178.\n",
      "[I 2025-09-25 21:25:13,721] Trial 23 finished with value: 0.9164066608931544 and parameters: {'n_estimators': 921, 'max_depth': 5, 'learning_rate': 0.030056636459159885, 'subsample': 0.7143351940366536, 'colsample_bytree': 0.7673713221208125, 'gamma': 3.484477969448779}. Best is trial 14 with value: 0.9209381796271178.\n",
      "[I 2025-09-25 21:25:14,593] Trial 24 finished with value: 0.916718916805915 and parameters: {'n_estimators': 818, 'max_depth': 4, 'learning_rate': 0.13723572360118497, 'subsample': 0.8344412723850613, 'colsample_bytree': 0.8282191540314462, 'gamma': 4.441107044156879}. Best is trial 14 with value: 0.9209381796271178.\n",
      "[I 2025-09-25 21:25:15,553] Trial 25 finished with value: 0.9214066367270073 and parameters: {'n_estimators': 683, 'max_depth': 3, 'learning_rate': 0.058732219260984986, 'subsample': 0.6499701449545705, 'colsample_bytree': 0.7081248122810748, 'gamma': 2.86437669145319}. Best is trial 25 with value: 0.9214066367270073.\n",
      "[I 2025-09-25 21:25:16,430] Trial 26 finished with value: 0.9192192342612109 and parameters: {'n_estimators': 635, 'max_depth': 4, 'learning_rate': 0.05818399208912415, 'subsample': 0.6051975099935029, 'colsample_bytree': 0.6906675273078955, 'gamma': 2.7195422227813464}. Best is trial 25 with value: 0.9214066367270073.\n",
      "[I 2025-09-25 21:25:17,470] Trial 27 finished with value: 0.9178132771155522 and parameters: {'n_estimators': 743, 'max_depth': 6, 'learning_rate': 0.06631524027557405, 'subsample': 0.6502624910377548, 'colsample_bytree': 0.7804546767416525, 'gamma': 2.7674154267089994}. Best is trial 25 with value: 0.9214066367270073.\n",
      "[I 2025-09-25 21:25:18,644] Trial 28 finished with value: 0.9146878619887451 and parameters: {'n_estimators': 558, 'max_depth': 5, 'learning_rate': 0.021345388531374698, 'subsample': 0.665994463227265, 'colsample_bytree': 0.6488949340971115, 'gamma': 3.617709142121409}. Best is trial 25 with value: 0.9214066367270073.\n",
      "[I 2025-09-25 21:25:21,328] Trial 29 finished with value: 0.9112502641799262 and parameters: {'n_estimators': 701, 'max_depth': 7, 'learning_rate': 0.01090869283778822, 'subsample': 0.6269659703779675, 'colsample_bytree': 0.8094282817566016, 'gamma': 2.2289933450145556}. Best is trial 25 with value: 0.9214066367270073.\n",
      "[I 2025-09-25 21:25:22,082] Trial 30 finished with value: 0.921094307583498 and parameters: {'n_estimators': 383, 'max_depth': 3, 'learning_rate': 0.04485935014391727, 'subsample': 0.7132366766366905, 'colsample_bytree': 0.8562470510097187, 'gamma': 1.9833211589447224}. Best is trial 25 with value: 0.9214066367270073.\n",
      "[I 2025-09-25 21:25:22,747] Trial 31 finished with value: 0.9195320760199616 and parameters: {'n_estimators': 325, 'max_depth': 3, 'learning_rate': 0.04319631057816875, 'subsample': 0.7083160351026964, 'colsample_bytree': 0.8822401195071918, 'gamma': 0.9677950466932075}. Best is trial 25 with value: 0.9214066367270073.\n",
      "[I 2025-09-25 21:25:23,726] Trial 32 finished with value: 0.9215626914526388 and parameters: {'n_estimators': 394, 'max_depth': 3, 'learning_rate': 0.05287429448990483, 'subsample': 0.6304223390051609, 'colsample_bytree': 0.9145906010490835, 'gamma': 2.7973935230347724}. Best is trial 32 with value: 0.9215626914526388.\n",
      "[I 2025-09-25 21:25:24,495] Trial 33 finished with value: 0.9210939414297542 and parameters: {'n_estimators': 393, 'max_depth': 3, 'learning_rate': 0.052232925696823695, 'subsample': 0.6273254918786488, 'colsample_bytree': 0.9251926477760581, 'gamma': 2.006927377672397}. Best is trial 32 with value: 0.9215626914526388.\n",
      "[I 2025-09-25 21:25:25,346] Trial 34 finished with value: 0.9218751670576456 and parameters: {'n_estimators': 379, 'max_depth': 3, 'learning_rate': 0.05188039719285174, 'subsample': 0.6319847214491869, 'colsample_bytree': 0.9312608309405273, 'gamma': 2.02586557015587}. Best is trial 34 with value: 0.9218751670576456.\n",
      "[I 2025-09-25 21:25:25,793] Trial 35 finished with value: 0.9132812457663473 and parameters: {'n_estimators': 214, 'max_depth': 3, 'learning_rate': 0.03570413171646584, 'subsample': 0.660983248671893, 'colsample_bytree': 0.9781923829970839, 'gamma': 1.894276853260892}. Best is trial 34 with value: 0.9218751670576456.\n",
      "[I 2025-09-25 21:25:26,306] Trial 36 finished with value: 0.9060939406974468 and parameters: {'n_estimators': 261, 'max_depth': 3, 'learning_rate': 0.02156426796113453, 'subsample': 0.6377422157798996, 'colsample_bytree': 0.9555015256045583, 'gamma': 2.6413870215320085}. Best is trial 34 with value: 0.9218751670576456.\n",
      "[I 2025-09-25 21:25:27,917] Trial 37 finished with value: 0.9128128618972067 and parameters: {'n_estimators': 510, 'max_depth': 8, 'learning_rate': 0.05267126808164117, 'subsample': 0.6796186136945112, 'colsample_bytree': 0.9040158086860862, 'gamma': 1.2141650570738103}. Best is trial 34 with value: 0.9218751670576456.\n",
      "[I 2025-09-25 21:25:28,567] Trial 38 finished with value: 0.917812984192557 and parameters: {'n_estimators': 394, 'max_depth': 4, 'learning_rate': 0.04094010585931227, 'subsample': 0.7788799686501527, 'colsample_bytree': 0.8593573248336609, 'gamma': 2.9366000589319072}. Best is trial 34 with value: 0.9218751670576456.\n",
      "[I 2025-09-25 21:25:29,935] Trial 39 finished with value: 0.914531367878621 and parameters: {'n_estimators': 416, 'max_depth': 8, 'learning_rate': 0.03016138924367729, 'subsample': 0.6545427864076329, 'colsample_bytree': 0.9128191746935754, 'gamma': 1.7698241342775982}. Best is trial 34 with value: 0.9218751670576456.\n",
      "[I 2025-09-25 21:25:30,588] Trial 40 finished with value: 0.9131250445792184 and parameters: {'n_estimators': 150, 'max_depth': 9, 'learning_rate': 0.047567690893986894, 'subsample': 0.620864342701794, 'colsample_bytree': 0.9542679970107537, 'gamma': 2.4021676729441435}. Best is trial 34 with value: 0.9218751670576456.\n",
      "[I 2025-09-25 21:25:31,187] Trial 41 finished with value: 0.9212505087706271 and parameters: {'n_estimators': 329, 'max_depth': 3, 'learning_rate': 0.05629540657267667, 'subsample': 0.6276317217724656, 'colsample_bytree': 0.9264559449325847, 'gamma': 2.0625993263057465}. Best is trial 34 with value: 0.9218751670576456.\n",
      "[I 2025-09-25 21:25:31,830] Trial 42 finished with value: 0.9209376670118763 and parameters: {'n_estimators': 328, 'max_depth': 3, 'learning_rate': 0.057049470856672335, 'subsample': 0.6373640913650425, 'colsample_bytree': 0.8830413764365691, 'gamma': 2.0659470255781454}. Best is trial 34 with value: 0.9218751670576456.\n",
      "[I 2025-09-25 21:25:32,333] Trial 43 finished with value: 0.9179691121489374 and parameters: {'n_estimators': 276, 'max_depth': 3, 'learning_rate': 0.03524225476450592, 'subsample': 0.6702192905128591, 'colsample_bytree': 0.8486866036144796, 'gamma': 1.4205135490460374}. Best is trial 34 with value: 0.9218751670576456.\n",
      "[I 2025-09-25 21:25:33,002] Trial 44 finished with value: 0.919844698086466 and parameters: {'n_estimators': 426, 'max_depth': 3, 'learning_rate': 0.08342338988151067, 'subsample': 0.9451646874645261, 'colsample_bytree': 0.9248649092994874, 'gamma': 0.8906768509969143}. Best is trial 34 with value: 0.9218751670576456.\n",
      "[I 2025-09-25 21:25:34,132] Trial 45 finished with value: 0.9160944782111428 and parameters: {'n_estimators': 529, 'max_depth': 4, 'learning_rate': 0.06315955230708781, 'subsample': 0.6151236562356724, 'colsample_bytree': 0.9682449465939643, 'gamma': 1.6546449621066683}. Best is trial 34 with value: 0.9218751670576456.\n",
      "[I 2025-09-25 21:25:34,734] Trial 46 finished with value: 0.9223441367727765 and parameters: {'n_estimators': 361, 'max_depth': 3, 'learning_rate': 0.04473318977161385, 'subsample': 0.7261920634301087, 'colsample_bytree': 0.9936332715982877, 'gamma': 2.5628952896392256}. Best is trial 46 with value: 0.9223441367727765.\n",
      "[I 2025-09-25 21:25:35,507] Trial 47 finished with value: 0.9145317340323648 and parameters: {'n_estimators': 280, 'max_depth': 4, 'learning_rate': 0.026547155259317626, 'subsample': 0.7560232700799927, 'colsample_bytree': 0.9974911950785068, 'gamma': 2.5307111523070356}. Best is trial 46 with value: 0.9223441367727765.\n",
      "[I 2025-09-25 21:25:36,261] Trial 48 finished with value: 0.9187508503920702 and parameters: {'n_estimators': 592, 'max_depth': 3, 'learning_rate': 0.12912281286999092, 'subsample': 0.7284510717910546, 'colsample_bytree': 0.9440904751780822, 'gamma': 2.2878434968636747}. Best is trial 46 with value: 0.9223441367727765.\n",
      "[I 2025-09-25 21:25:37,381] Trial 49 finished with value: 0.9168753376852902 and parameters: {'n_estimators': 450, 'max_depth': 6, 'learning_rate': 0.037808459526337004, 'subsample': 0.6399631303294653, 'colsample_bytree': 0.9716016109085247, 'gamma': 2.867507622459131}. Best is trial 46 with value: 0.9223441367727765.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Trial Score: 0.9223\n",
      "Best XGBoost Params: {'n_estimators': 361, 'max_depth': 3, 'learning_rate': 0.04473318977161385, 'subsample': 0.7261920634301087, 'colsample_bytree': 0.9936332715982877, 'gamma': 2.5628952896392256}\n",
      "\n",
      "Optimized XGBoost Performance:\n",
      "Accuracy: 0.9250\n",
      "F1 Score: 0.9587\n",
      "ROC-AUC: 0.8238\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "explanations": [
     "This cell performs hyperparameter optimization for the Keras Deep Learning model using Optuna. A function creates the Keras model with hyperparameters suggested by the Optuna trial. The objective function then trains and evaluates this model, returning the validation accuracy. Optuna intelligently searches for the best network architecture and learning rate."
    ],
    "ExecuteTime": {
     "end_time": "2025-09-25T16:00:42.150767Z",
     "start_time": "2025-09-25T15:55:38.647978Z"
    }
   },
   "source": [
    "print(\"\\n--- 8.2. Optuna for Keras NN ---\")\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "def create_keras_model(trial):\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(trial.suggest_int('units_0', 32, 256), activation='relu', input_shape=(X_train_processed.shape[1],)))\n",
    "    model.add(Dropout(trial.suggest_float('dropout_0', 0.2, 0.5)))\n",
    "    \n",
    "    for i in range(1, n_layers):\n",
    "        model.add(Dense(trial.suggest_int(f'units_{i}', 16, 128), activation='relu'))\n",
    "        model.add(Dropout(trial.suggest_float(f'dropout_{i}', 0.2, 0.5)))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def objective_keras(trial):\n",
    "    model = create_keras_model(trial)\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "    history = model.fit(X_train_processed, y_train, epochs=100, batch_size=trial.suggest_int('batch_size', 32, 128),\n",
    "                        validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "    return history.history['val_accuracy'][-1]\n",
    "\n",
    "study_keras = optuna.create_study(direction='maximize')\n",
    "study_keras.optimize(objective_keras, n_trials=30, timeout=600)\n",
    "\n",
    "print(f\"Best Keras Trial Score: {study_keras.best_value:.4f}\")\n",
    "print(f\"Best Keras Params: {study_keras.best_params}\")\n",
    "\n",
    "# Retrain and evaluate with best params\n",
    "best_keras_model = create_keras_model(study_keras.best_trial)\n",
    "best_keras_model.fit(X_train_processed, y_train, epochs=100, batch_size=study_keras.best_params['batch_size'],\n",
    "                     validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "y_proba = best_keras_model.predict(X_test_processed).ravel()\n",
    "y_pred = (y_proba > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nOptimized Keras Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")"
   ],
   "id": "optuna_keras",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-25 21:25:38,692] A new study created in memory with name: no-name-f36483b2-fe7b-482a-9047-d7ea88d55c35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 8.2. Optuna for Keras NN ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-25 21:25:57,366] Trial 0 finished with value: 0.9117187261581421 and parameters: {'n_layers': 2, 'units_0': 107, 'dropout_0': 0.2965391203351117, 'units_1': 71, 'dropout_1': 0.40652073619887014, 'learning_rate': 0.00043247438531843164, 'batch_size': 66}. Best is trial 0 with value: 0.9117187261581421.\n",
      "[I 2025-09-25 21:26:09,445] Trial 1 finished with value: 0.91796875 and parameters: {'n_layers': 1, 'units_0': 129, 'dropout_0': 0.26482651511882055, 'learning_rate': 0.004605986592202893, 'batch_size': 124}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:26:32,237] Trial 2 finished with value: 0.9140625 and parameters: {'n_layers': 2, 'units_0': 248, 'dropout_0': 0.2747948899248066, 'units_1': 123, 'dropout_1': 0.46991611129305066, 'learning_rate': 0.00012233143258915207, 'batch_size': 68}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:26:39,828] Trial 3 finished with value: 0.910937488079071 and parameters: {'n_layers': 3, 'units_0': 161, 'dropout_0': 0.42433288506170297, 'units_1': 18, 'dropout_1': 0.46352861047816735, 'units_2': 67, 'dropout_2': 0.22312954252163777, 'learning_rate': 0.004052592051481833, 'batch_size': 114}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:26:52,411] Trial 4 finished with value: 0.913281261920929 and parameters: {'n_layers': 1, 'units_0': 91, 'dropout_0': 0.42427278695760196, 'learning_rate': 0.005390021224201046, 'batch_size': 53}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:27:04,664] Trial 5 finished with value: 0.9078124761581421 and parameters: {'n_layers': 3, 'units_0': 43, 'dropout_0': 0.3008923024365505, 'units_1': 110, 'dropout_1': 0.2677266594296587, 'units_2': 28, 'dropout_2': 0.2406007192972963, 'learning_rate': 0.0013345753422887596, 'batch_size': 112}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:27:21,343] Trial 6 finished with value: 0.90625 and parameters: {'n_layers': 2, 'units_0': 95, 'dropout_0': 0.3145186823934592, 'units_1': 91, 'dropout_1': 0.3261456438266076, 'learning_rate': 0.00031937707944528843, 'batch_size': 38}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:27:26,432] Trial 7 finished with value: 0.8804687261581421 and parameters: {'n_layers': 3, 'units_0': 73, 'dropout_0': 0.2829227595076705, 'units_1': 17, 'dropout_1': 0.22767938504860416, 'units_2': 37, 'dropout_2': 0.27678764094098207, 'learning_rate': 0.0002832967370025172, 'batch_size': 73}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:27:30,889] Trial 8 finished with value: 0.8804687261581421 and parameters: {'n_layers': 2, 'units_0': 200, 'dropout_0': 0.2999164703220457, 'units_1': 19, 'dropout_1': 0.4501547404994671, 'learning_rate': 0.0001250096700381813, 'batch_size': 99}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:27:49,144] Trial 9 finished with value: 0.8999999761581421 and parameters: {'n_layers': 1, 'units_0': 103, 'dropout_0': 0.3814134813199679, 'learning_rate': 0.00013833754507620833, 'batch_size': 106}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:27:57,096] Trial 10 finished with value: 0.914843738079071 and parameters: {'n_layers': 1, 'units_0': 153, 'dropout_0': 0.2087548591867125, 'learning_rate': 0.001837523692996325, 'batch_size': 126}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:28:02,306] Trial 11 finished with value: 0.910937488079071 and parameters: {'n_layers': 1, 'units_0': 155, 'dropout_0': 0.20142103806379558, 'learning_rate': 0.0019467806099498905, 'batch_size': 128}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:28:05,959] Trial 12 finished with value: 0.91015625 and parameters: {'n_layers': 1, 'units_0': 186, 'dropout_0': 0.20398071421121955, 'learning_rate': 0.009735760715695665, 'batch_size': 128}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:28:17,398] Trial 13 finished with value: 0.9164062738418579 and parameters: {'n_layers': 1, 'units_0': 129, 'dropout_0': 0.49994217987230816, 'learning_rate': 0.0025862981927510962, 'batch_size': 89}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:28:22,583] Trial 14 finished with value: 0.91015625 and parameters: {'n_layers': 1, 'units_0': 126, 'dropout_0': 0.49861486242216446, 'learning_rate': 0.003547975178094702, 'batch_size': 89}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:28:34,657] Trial 15 finished with value: 0.909375011920929 and parameters: {'n_layers': 2, 'units_0': 60, 'dropout_0': 0.49515730093692745, 'units_1': 55, 'dropout_1': 0.34330657071790627, 'learning_rate': 0.000869255063164155, 'batch_size': 91}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:28:41,134] Trial 16 finished with value: 0.9125000238418579 and parameters: {'n_layers': 1, 'units_0': 129, 'dropout_0': 0.24923498626820142, 'learning_rate': 0.00918878146942318, 'batch_size': 83}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:28:55,549] Trial 17 finished with value: 0.914843738079071 and parameters: {'n_layers': 1, 'units_0': 182, 'dropout_0': 0.3607878978306754, 'learning_rate': 0.002833731510096096, 'batch_size': 49}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:29:05,182] Trial 18 finished with value: 0.910937488079071 and parameters: {'n_layers': 1, 'units_0': 226, 'dropout_0': 0.44326843546863054, 'learning_rate': 0.0009387076830650418, 'batch_size': 116}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:29:12,532] Trial 19 finished with value: 0.9085937738418579 and parameters: {'n_layers': 2, 'units_0': 130, 'dropout_0': 0.34272993302138727, 'units_1': 46, 'dropout_1': 0.21089957817495647, 'learning_rate': 0.006185540581186489, 'batch_size': 101}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:29:20,376] Trial 20 finished with value: 0.9085937738418579 and parameters: {'n_layers': 2, 'units_0': 175, 'dropout_0': 0.24325421391757834, 'units_1': 93, 'dropout_1': 0.29382964644757786, 'learning_rate': 0.002287004863863154, 'batch_size': 78}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:29:27,519] Trial 21 finished with value: 0.9117187261581421 and parameters: {'n_layers': 1, 'units_0': 145, 'dropout_0': 0.22978691118517997, 'learning_rate': 0.0017668406217208545, 'batch_size': 120}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:29:38,282] Trial 22 finished with value: 0.91015625 and parameters: {'n_layers': 1, 'units_0': 117, 'dropout_0': 0.25662169718324507, 'learning_rate': 0.0006932314644326898, 'batch_size': 123}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:29:47,597] Trial 23 finished with value: 0.9140625 and parameters: {'n_layers': 1, 'units_0': 207, 'dropout_0': 0.2255509003237394, 'learning_rate': 0.0014295798990784133, 'batch_size': 107}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:29:54,169] Trial 24 finished with value: 0.910937488079071 and parameters: {'n_layers': 1, 'units_0': 146, 'dropout_0': 0.3277447432360321, 'learning_rate': 0.0032522941835219934, 'batch_size': 98}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:30:01,301] Trial 25 finished with value: 0.91796875 and parameters: {'n_layers': 1, 'units_0': 165, 'dropout_0': 0.3799032633333326, 'learning_rate': 0.0060337153357324655, 'batch_size': 58}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:30:10,612] Trial 26 finished with value: 0.914843738079071 and parameters: {'n_layers': 1, 'units_0': 169, 'dropout_0': 0.46022968059956937, 'learning_rate': 0.005824452883787842, 'batch_size': 56}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:30:20,824] Trial 27 finished with value: 0.903124988079071 and parameters: {'n_layers': 2, 'units_0': 79, 'dropout_0': 0.39024844877636994, 'units_1': 41, 'dropout_1': 0.39931958816546553, 'learning_rate': 0.004685844562641521, 'batch_size': 37}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:30:26,905] Trial 28 finished with value: 0.91015625 and parameters: {'n_layers': 1, 'units_0': 134, 'dropout_0': 0.47187164884764204, 'learning_rate': 0.006747287031022352, 'batch_size': 58}. Best is trial 1 with value: 0.91796875.\n",
      "[I 2025-09-25 21:30:36,185] Trial 29 finished with value: 0.905468761920929 and parameters: {'n_layers': 2, 'units_0': 117, 'dropout_0': 0.3912552915464251, 'units_1': 75, 'dropout_1': 0.49808844852808704, 'learning_rate': 0.0025454070597844053, 'batch_size': 43}. Best is trial 1 with value: 0.91796875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Keras Trial Score: 0.9180\n",
      "Best Keras Params: {'n_layers': 1, 'units_0': 129, 'dropout_0': 0.26482651511882055, 'learning_rate': 0.004605986592202893, 'batch_size': 124}\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step \n",
      "\n",
      "Optimized Keras Performance:\n",
      "Accuracy: 0.8906\n",
      "F1 Score: 0.9406\n",
      "ROC-AUC: 0.7680\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
